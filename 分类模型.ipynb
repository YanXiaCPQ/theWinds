{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "1. 文件读取 \n",
    "2. 特征提取\n",
    "3. 分类算法，svm,RF,\n",
    "4. 绘图PR，AUC，训练曲线与测试曲线，学习曲线与验证曲线\n",
    "5. 指标acc,presion,recall,自定义的误判率和漏判率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. 33.]\n",
      " [ 0.  1.  0. 12.]\n",
      " [ 0.  0.  1. 18.]] \n",
      "\n",
      "city有三个字符型的值，one-hot后有三列;temperature数值型，保留，所以共有4列 \n",
      "\n",
      "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'] \n",
      "\n",
      "通过此方法获得one-hot后，每列所对应的原本的值\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# 将dict转成numpy。采用了对同一个key采用了ont-hot，数值型values保留原类型\n",
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Francisco', 'temperature': 18.},\n",
    "] \n",
    "\n",
    "vec = DictVectorizer()\n",
    "print(vec.fit_transform(measurements).toarray(), '\\n')\n",
    "print('city有三个字符型的值，one-hot后有三列;temperature数值型，保留，所以共有4列','\\n')\n",
    "print(vec.get_feature_names(), '\\n')\n",
    "print('通过此方法获得one-hot后，每列所对应的原本的值')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "- 实现了 tokenization （词语切分,包括n-gram）和 occurrence counting （出现频数统计）,TF-IDF\n",
    "- 这个模型有很多参数，但参数的默认初始值是相当合理的。查看文档获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 实现了 tokenization （词语切分,包括n-gram,停用词）和 occurrence counting （出现频数统计）,TF-IDF\n",
    "# 这个模型有很多参数，但参数的默认初始值是相当合理的。查看文档获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "#bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "#                                    token_pattern=r'\\b\\w+\\b', min_df=1) # 提取n-gram\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray() # 词袋结果, 注意第二行有个2 ，说明了该方法是统计了词频的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_ # 查看单词对应索引\n",
    "vectorizer.vocabulary_.get('xxx') # 指定获取某一单词的所有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81940995, 0.        , 0.57320793],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.47330339, 0.88089948, 0.        ],\n",
       "       [0.58149261, 0.        , 0.81355169]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674],\n",
       "       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,\n",
       "        0.85322574, 0.22262429, 0.        , 0.27230147],\n",
       "       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
       "        0.        , 0.28847675, 0.55280532, 0.        ],\n",
       "       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 它将 CountVectorizer 和 TfidfTransformer 的所有选项组合在一个单例模型中\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC(C=1.0)\n",
    "clf.fit(X, y)\n",
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "      \n",
    "def file_name(file_dir):   \n",
    "    for root, dirs, files in os.walk(file_dir):  \n",
    "        #print(root) #当前目录路径  \n",
    "        #print(dirs) #当前路径下所有子目录  \n",
    "        return files #当前路径下所有非目录子文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg类有12500个样本\n",
      "pos类有12500个样本\n"
     ]
    }
   ],
   "source": [
    "list_fileName_neg = file_name('./dataSet/aclImdb/train/neg')# 文件名列表\n",
    "list_fileName_pos = file_name('./dataSet/aclImdb/train/pos')# 文件名列表\n",
    "print('neg类有%d个样本'%len(list_fileName_neg))\n",
    "print('pos类有%d个样本'%len(list_fileName_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据路径和文件名列表读取文件，用list存储\n",
    "def get_list_fileData(file_dir=None, list_fileName = None):\n",
    "    if file_dir is None or file_dir ==\" \":\n",
    "        print('文件路径为空')\n",
    "        return None\n",
    "    if list_fileName is None or list_fileName ==\" \":\n",
    "        print('文件名列表为空')\n",
    "        return None\n",
    "    fileData = []\n",
    "    for file in list_fileName:\n",
    "        with open(file_dir+file, encoding='utf-8') as f:\n",
    "            fileData.append(f.read())\n",
    "    return fileData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData_neg = get_list_fileData(file_dir='./dataSet/aclImdb/train/neg/', list_fileName = list_fileName_neg)\n",
    "fileData_pos = get_list_fileData(file_dir='./dataSet/aclImdb/train/pos/', list_fileName = list_fileName_pos)\n",
    "label_neg = list(['__label__%s'%'neg']*len(fileData_neg))\n",
    "label_pos = list(['__label__%s'%'pos']*len(fileData_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 74849)\n"
     ]
    }
   ],
   "source": [
    "# 这一步及其消耗内存，随着数据量增加，飞速增长。实验时大概要6G。\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 它将 CountVectorizer 和 TfidfTransformer 的所有选项组合在一个单例模型中\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_feature = fileData_neg + fileData_pos\n",
    "feature = vectorizer.fit_transform(train_feature).toarray()\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "del vectorizer, fileData_neg, fileData_pos\n",
    "gc.collect()\n",
    "# 少3G内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del feature\n",
    "gc.collect()\n",
    "# 少2G内存"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
